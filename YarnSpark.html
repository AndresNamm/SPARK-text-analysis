<hr />

<p>layout: "post"
title: "GP_Andres"</p>

<h2 id="date201704191259">date: "2017-04-19 12:59"</h2>

<p><strong>Table of Contents</strong>  <em>generated with <a href="http://doctoc.herokuapp.com/">DocToc</a></em></p>

<ul>
<li><a href="#">Text Analysis Project</a>


<ul>
<li><a href="#">Intro</a></li>

<li><a href="#">Setting up Spark in Yarn</a>


<ul>
<li><a href="#">Recap of Definitions in Hadoop</a></li>

<li><a href="#">Definitions for Hadoop and Yarn</a></li>

<li><a href="#">Starting spark</a></li>

<li><a href="#">Cluster division studentX - dom0, studentX-xX - vm</a></li>

<li><a href="#">Important settings</a></li>

<li><a href="#">Problems</a></li>

<li><a href="#">Logging</a></li>

<li><a href="#">Managing the CLUSTER</a></li></ul>
</li>

<li><a href="#">Ipython Notebook</a>


<ul>
<li><a href="#">Installation</a></li>

<li><a href="#">Starting Pyspark</a></li></ul>
</li>

<li><a href="#">Spark Tutorials Python</a></li>

<li><a href="#">Spark on Yarn Architecture</a></li></ul>
</li>
</ul>

<h1 id="textanalysisproject">Text Analysis Project</h1>

<p><strong>All info related to this project is available at Github</strong>: <a href="https://github.com/AndresNamm/SPARK-text-analysis.git">Project setup</a> ; <a href="https://github.com/AndresNamm/NgramAnalysis.git">Project Source</a></p>

<h2 id="intro">Intro</h2>

<p>We Implemented google <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">N-GRAM</a> 
Analysis with Spark on Yarn. This is a short summary of the problem we solved and issues that occured and how we solved them. 
I have also included some references to good tutorials and I hope some of my insights can help people who are solving different problems than this. 
The analysis we did is rather simple, but it is a good example, because it involves a large dataset, which takes a lot of time to analyse. This allows 
us to take advantage of a lot of features of spark  withou focusing too much on the complexities of underlying algorithms. <br />
For example The <strong>execution improvements</strong> we got from adding extra memory to executors and adding extra nodes reduced the time of finding
x most popular words from each year ( map and reduce job on all 5.2 GB of data ) from 40 minutes to 30 minutes. It is a clear example of parallelism. </p>

<p>Dataset: google 1gram;science-fiction books;5.2 GB of data <br />
Enviroment: Cluster with 5 nodes &lt;- 1 master and 4 slaves. </p>

<h2 id="settingupsparkinyarn">Setting up Spark in Yarn</h2>

<h3 id="recapofdefinitionsinhadoop">Recap of Definitions in Hadoop</h3>

<ul>
<li>RESOURCE MANAGER (YARN) - manages all resources :O</li>

<li>NODE MANAGER (HADOOP) - manages file system</li>

<li>APPLICATION MASTER - For every application, there exists 1 master. concept in yarn. Allows to run multiple applications at the same time</li>

<li>CONTAINER - resizabel. useful for spark.</li>
</ul>

<h3 id="definitionsforhadoopandyarn">Definitions for Hadoop and Yarn</h3>

<ul>
<li>CLIENT VS CLUSTER modified


<ul>
<li>CLIENT MODE is when the driver is running inside the client process. (The one, who initiated application)</li>

<li>CLUSTER MODE is when the driver is running inside Application master in yarn</li></ul>
</li>

<li>APPLICATION


<ul>
<li>JOB - like in yarn , runs inside application</li>

<li>TASK - like in yarn</li></ul>
</li>

<li>DRIVER PROGRAM - Schdules tasks on Executors, Manages job flow</li>

<li>CLUSTER MANAGER - Starts executor processes</li>

<li>PARTITION - like in HDFS</li>

<li>SPARK-SUBMIT vs SPARK-SHELL/PYSPART


<ul>
<li>SUBMIT - For scripts</li>

<li>SHELL - for shell :O</li></ul>
</li>

<li>TRANSFORMATION, ACTION


<ul>
<li>TRANSFORMATION</li>

<li>for example map</li>

<li>ACTION</li>

<li>for example reduce</li></ul>
</li>

<li>RDD


<ul>
<li>PARTITIONS - blocksize in hdfs. More => More paralelism.</li></ul>
</li>
</ul>

<h3 id="startingspark">Starting spark</h3>

<p>For debugging I guess its always easier to start Spark Context with speficic settings on the command line like in this example:</p>

<blockquote>
  <p>pyspark --master yarn-cluster [--driver-memory 4g]   -- for python with settings</p>
</blockquote>

<h3 id="clusterdivisionstudentxdom0studentxxxvm">Cluster division studentX - dom0, studentX-xX - vm</h3>

<ul>
<li>student1 – Ganglia    </li>

<li>student1-x1 project webpage    </li>

<li>student1-x2 project administration page    </li>

<li>student13-x1 Hadoop master  </li>

<li>student85-x1 slave  </li>

<li>student85-x2 slave  </li>

<li>student13-x2 slave  </li>

<li>student14-x1 slave  </li>

<li>student14-x2 slave  </li>
</ul>

<h3 id="importantsettings">Important settings</h3>

<p>Because spark uses the principle, that all executors have to share same settings, I configured yarn to be the same across all nodes and decommissioned nodes, 
that would have become bottlenecks with decommissioning procedure. This also allows easier cluster management.</p>

<p><strong>spark.defaults</strong> - spark setting when you dont specify anything else8 in the commandline or in the program</p>

<blockquote>
  <p>spark.executor.instances         4 <br />
  spark.yarn.archive         hdfs://student13-x1:9000/addedlibs/spark-archive.zip <br />
  spark.executor.memory         4g <br />
  spark.driver.memory         4g <br />
  spark.yarn.executor.memoryOverhead 700 <br />
  spark.yarn.driver.memoryOverhead   1000 <br />
  spark.yarn.am.memoryOverhead       700   </p>
</blockquote>

<p><strong>yarn-site.xml</strong> </p>

<blockquote>
  <p><property> <br />
          <name>yarn.nodemanager.resource.memory-mb</name> <br />
          <value>5020</value> <br />
  </property> <br />
  <property> <br />
          <name>yarn.scheduler.minimum-allocation-mb</name> <br />
          <value>1024</value> <br />
  </property> <br />
  <property> <br />
          <name>yarn.scheduler.maximum-allocation-mb</name> <br />
          <value>5020</value> <br />
  </property> <br />
  <property> <br />
    <name>yarn.resourcemanager.nodes.exclude-path</name> <br />
    <value>/opt/hadoop-2.6.0/etc/hadoop/yarn.exclude</value> <br />
    <final>true</final> <br />
  </property>  </p>
</blockquote>

<h3 id="problems">Problems</h3>

<p>Error TimeLine       </p>

<ol>
<li>The Spark Shell starts</li>

<li>Error – Slave Lost – Happened with diferent slaves, with Scala and Pyspark. Yarn container logs dont show any memory issues. </li>
</ol>

<p>Turns out the error comes from java 8 using too much VM which does not fit into the bounds of Spark boundaries. These settings made all the errors
disappear.</p>

<blockquote>
  <p>spark.yarn.executor.memoryOverhead 700 <br />
  spark.yarn.driver.memoryOverhead   1000 <br />
  spark.yarn.am.memoryOverhead       700    </p>
</blockquote>

<ul>
<li>http://stackoverflow.com/questions/28967500/why-spark-application-fail-with-executor-coarsegrainedexecutorbackend-driver-d  </li>

<li>http://stackoverflow.com/questions/27792839/spark-fail-when-running-pi-py-example-with-yarn-client-mode</li>

<li>http://stackoverflow.com/questions/28967500/why-spark-application-fail-with-executor-coarsegrainedexecutorbackend-driver-d</li>

<li>http://stackoverflow.com/questions/33141129/why-spark-executor-receives-sigterm</li>
</ul>

<h3 id="logging">Logging</h3>

<p>Overally it seems like Hadoop logs are more specific regarding resources like physical and virtual memory whereas spark logs the events
and their execution sequences, e.g. it is more abstract.</p>

<p>You can find container logs under  the nodes , which ran the container
For example in this project container logs for student13-x1. The ip naturally 
http://[IP]/logs/userlogs/application<em>1490663309971</em>0002/container<em>1490663309971</em>0002<em>01</em>000001/</p>

<h3 id="managingthecluster">Managing the CLUSTER</h3>

<p>If you have multiple machines, then it adds a lot of extra work and considerations for you in regards to managing the environment.  To ease that load
I created</p>

<ul>
<li>THIS REFERENCE GUIDE - helps to keep a summary of the most importand aspect in your project. </li>

<li>SCRIPTS - To ease the management of services I wrote 3 scripts: startHadoop: Restarts the hadoop environment; shareSettings: shares hadoop settings across specified nodes;
shareSpark: shares spark settings across the cluster.  </li>

<li>GITHUB - allows easily to get access to important settings to analize them. Can store the development history. Good intermediate between the cluster and other computers   </li>

<li><a href="http://ganglia.sourceforge.net/">GANGLIA</a> - Provides simple interface to have an overall view of resource usages.</li>

<li>ListOflinks - It's good to keep a simple website which references all the web interfaces you have in the project. Otherwise it's easy to loose track </li>
</ul>

<h2 id="ipythonnotebook">Ipython Notebook</h2>

<h3 id="installation">Installation</h3>

<ul>
<li>I had to install pip and ipython jupyter according to this <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-jupyter-notebook-to-run-ipython-on-ubuntu-16-04">tutorial</a> and then perform tunneling, which was specific to my envrionment.
Notebook is accessible, when its running in port 18813</li>

<li>Now I have to try to configure spark and both iphyhton so spark context could be commanded in Ipython Environment
I did the installation exactly like in this [tutorial](http://coolinux.com/blog/2015/07/24/install-pyspark-and-integrate-ipython/
) except for the</li>
</ul>

<blockquote>
  <p>PYSPARK<em>DRIVER</em>PYTHON_OPTS='notebook --ip=* ' pyspark'</p>
</blockquote>

<p>I also had to specify pyspark in .profile file in my home directory to be a path variable</p>

<h3 id="startingpyspark">Starting Pyspark</h3>

<ul>
<li>Now, to start ipython together with spark just execute at hduser@student13-x1</li>
</ul>

<blockquote>
  <p>pyspark --master yarn [other options ]          </p>
</blockquote>

<p>at you home directory or wherever you find this most reasonable. This starts a Ipython notebook in address http://202.45.128.135:18813/?token=dsmaofnfds
(token is different every time naturally)</p>

<h2 id="sparktutorialspython">Spark Tutorials Python</h2>

<ul>
<li><a href="https://docs.databricks.com/spark/latest/training/index.html">I think this is the most comprehensive tutorial, but its long.</a></li>

<li><a href="https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html">Spark job Visualisation</a></li>

<li>http://spark.apache.org/docs/latest/running-on-yarn.html</li>

<li>https://www.supergloo.com/spark-tutorial/spark-tutorials-python/</li>

<li>https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python</li>

<li>https://www.dezyre.com/apache-spark-tutorial/pyspark-tutorial</li>

<li>https://www.datacamp.com/community/tutorials/apache-spark-python#gs.9_=gwlg</li>

<li>http://spark.apache.org/docs/latest/quick-start.html</li>

<li>https://mapr.com/ebooks/spark/05-processing-tabular-data-with-spark-sql.html</li>
</ul>

<h2 id="sparkonyarnarchitecture">Spark on Yarn Architecture</h2>

<ul>
<li>http://badrit.com/blog/2015/2/29/running-spark-on-yarn#.WNSeW6IlGUl</li>

<li>http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/  </li>

<li>https://jaceklaskowski.gitbooks.io/mastering-apache-spark</li>

<li>https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh<em>ig</em>running<em>spark</em>on_yarn.html</li>

<li>http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/</li>

<li>http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/</li>
</ul>